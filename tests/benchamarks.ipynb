{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the parent directory to the Python path\n",
    "import sys\n",
    "import os\n",
    "parent_dir = os.path.abspath('..')\n",
    "if parent_dir not in sys.path:\n",
    "    sys.path.append(parent_dir)\n",
    "\n",
    "# Import loss function\n",
    "from texstat.functions import *\n",
    "import texstat.torch_filterbanks.filterbanks as fb\n",
    "\n",
    "# Import extra packages\n",
    "import numpy as np\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Audio\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchaudio\n",
    "import time\n",
    "import gc\n",
    "\n",
    "# Pick device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_loss_function(loss_fn, input_shape=(32, 100), dtype=torch.float32, device='cuda', iterations=10, **loss_kwargs):\n",
    "    \"\"\"Benchmarks a given loss function for time and GPU memory usage.\"\"\"\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()  # Garbage collection to free up memory before benchmarking\n",
    "    \n",
    "    # Create dummy inputs and targets\n",
    "    inputs = torch.randn(*input_shape, dtype=dtype, device=device, requires_grad=True)\n",
    "    targets = torch.randn(*input_shape, dtype=dtype, device=device)\n",
    "    \n",
    "    optimizer = optim.SGD([inputs], lr=0.01)\n",
    "    \n",
    "    computation_times = []\n",
    "    grad_descent_times = []\n",
    "    memory_usages = []\n",
    "    \n",
    "    for _ in range(iterations):\n",
    "        torch.cuda.synchronize()\n",
    "        start_mem = torch.cuda.memory_allocated(device)\n",
    "        \n",
    "        # Measure computation time\n",
    "        start_time = time.time()\n",
    "        loss = loss_fn(inputs, targets, **loss_kwargs)\n",
    "        torch.cuda.synchronize()\n",
    "        end_time = time.time()\n",
    "        computation_time = end_time - start_time\n",
    "        \n",
    "        # Measure gradient descent time\n",
    "        start_time = time.time()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        torch.cuda.synchronize()\n",
    "        end_time = time.time()\n",
    "        grad_descent_time = end_time - start_time\n",
    "        \n",
    "        end_mem = torch.cuda.memory_allocated(device)\n",
    "        \n",
    "        computation_times.append(computation_time)\n",
    "        grad_descent_times.append(grad_descent_time)\n",
    "        memory_usages.append(end_mem - start_mem)\n",
    "    \n",
    "    # Compute mean and standard deviation\n",
    "    mean_computation_time = np.mean(computation_times)\n",
    "    std_computation_time = np.std(computation_times)\n",
    "    mean_grad_descent_time = np.mean(grad_descent_times)\n",
    "    std_grad_descent_time = np.std(grad_descent_times)\n",
    "    mean_memory = np.mean(memory_usages) / 1e6  # Convert to MB\n",
    "    std_memory = np.std(memory_usages) / 1e6  # Convert to MB\n",
    "    \n",
    "    print(f\"Computation Time:      {mean_computation_time:.6f} sec (±{std_computation_time:.6f})\")\n",
    "    print(f\"Gradient Descent Time: {mean_grad_descent_time:.6f} sec (±{std_grad_descent_time:.6f})\")\n",
    "    print(f\"Memory Usage:          {mean_memory:.2f} MB (±{std_memory:.2f})\\n\")\n",
    "    \n",
    "    return mean_computation_time, std_computation_time, mean_grad_descent_time, std_grad_descent_time, mean_memory, std_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for TexStat\n",
    "sr, frame_size = 44100, 2**16\n",
    "N_filter_bank = 16\n",
    "M_filter_bank = 6\n",
    "N_moments     = 4\n",
    "alpha         = torch.tensor([100, 1, 1/10, 1/100], device=device)\n",
    "beta  = torch.tensor([1, 1, 1, 1, 1], device=device)\n",
    "new_sr, new_frame_size = sr // 4, frame_size // 4\n",
    "downsampler = torchaudio.transforms.Resample(sr, new_sr).to(device)\n",
    "coch_fb = fb.EqualRectangularBandwidth(frame_size, sr, N_filter_bank, 20, sr // 2)\n",
    "mod_fb  = fb.Logarithmic(new_frame_size, new_sr, M_filter_bank, 10, new_sr // 4)\n",
    "\n",
    "def custom_texstat_loss(x, y, coch_fb, mod_fb, downsampler, N_moments, alpha, beta):\n",
    "    return texstat_loss(x, y, coch_fb, mod_fb, downsampler, N_moments, alpha, beta)\n",
    "\n",
    "# Running single computation benchmark\n",
    "print(\"TexStat single computation benchmark:\")\n",
    "benchmark_loss_function(custom_texstat_loss, input_shape=(1, frame_size), device=device,\n",
    "                        coch_fb=coch_fb, mod_fb=mod_fb, downsampler=downsampler,\n",
    "                        N_moments=N_moments, alpha=alpha, beta=beta)\n",
    "\n",
    "# Running batch computation benchmark\n",
    "print(\"TexStat batch computation benchmark:\")\n",
    "benchmark_loss_function(custom_texstat_loss, input_shape=(32, frame_size), device=device,\n",
    "                        coch_fb=coch_fb, mod_fb=mod_fb, downsampler=downsampler,\n",
    "                        N_moments=N_moments, alpha=alpha, beta=beta)\n",
    "\n",
    "print(\"Benchmarking done.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
