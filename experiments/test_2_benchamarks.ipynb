{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Add the parent directory to the Python path\n",
    "import sys\n",
    "import os\n",
    "parent_dir = os.path.abspath('..')\n",
    "if parent_dir not in sys.path:\n",
    "    sys.path.append(parent_dir)\n",
    "\n",
    "# Import loss function\n",
    "from texstat.functions import *\n",
    "import texstat.torch_filterbanks.filterbanks as fb\n",
    "\n",
    "# Import extra packages\n",
    "import numpy as np\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Audio\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchaudio\n",
    "import time\n",
    "import gc\n",
    "\n",
    "# Pick device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_loss_function(loss_fn, input_shape=(32, 100), dtype=torch.float32, device='cuda', iterations=10, **loss_kwargs):\n",
    "    \"\"\"Benchmarks a given loss function for time and GPU memory usage.\"\"\"\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()  # Garbage collection to free up memory before benchmarking\n",
    "    \n",
    "    # Create dummy inputs and targets\n",
    "    inputs  = torch.randn(*input_shape, dtype=dtype, device=device, requires_grad=True)\n",
    "    targets = torch.randn(*input_shape, dtype=dtype, device=device)\n",
    "    \n",
    "    optimizer = optim.SGD([inputs], lr=0.01)\n",
    "    \n",
    "    computation_times = []\n",
    "    grad_descent_times = []\n",
    "    memory_usages = []\n",
    "    \n",
    "    for _ in range(iterations):\n",
    "        torch.cuda.synchronize()\n",
    "        start_mem = torch.cuda.memory_allocated(device)\n",
    "        \n",
    "        # Measure computation time\n",
    "        start_time = time.time()\n",
    "        loss = loss_fn(inputs, targets, **loss_kwargs)\n",
    "        torch.cuda.synchronize()\n",
    "        end_time = time.time()\n",
    "        computation_time = end_time - start_time\n",
    "        \n",
    "        # Measure gradient descent time\n",
    "        start_time = time.time()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        torch.cuda.synchronize()\n",
    "        end_time = time.time()\n",
    "        grad_descent_time = end_time - start_time\n",
    "        \n",
    "        end_mem = torch.cuda.memory_allocated(device)\n",
    "        \n",
    "        computation_times.append(computation_time)\n",
    "        grad_descent_times.append(grad_descent_time)\n",
    "        memory_usages.append(end_mem - start_mem)\n",
    "    \n",
    "    # Compute mean and standard deviation\n",
    "    mean_computation_time = np.mean(computation_times)\n",
    "    std_computation_time = np.std(computation_times)\n",
    "    mean_grad_descent_time = np.mean(grad_descent_times)\n",
    "    std_grad_descent_time = np.std(grad_descent_times)\n",
    "    mean_memory = np.mean(memory_usages) / 1e6  # Convert to MB\n",
    "    std_memory = np.std(memory_usages) / 1e6  # Convert to MB\n",
    "    \n",
    "    print(f\"Computation Time:      {mean_computation_time*1000:.3f} ms (±{std_computation_time*1000:.3f} ms)\")\n",
    "    print(f\"Gradient Descent Time: {mean_grad_descent_time*1000:.3f} ms (±{std_grad_descent_time*1000:.6f} ms)\")\n",
    "    print(f\"Memory Usage:          {mean_memory:.2f} MB (±{std_memory:.2f})\\n\")\n",
    "    \n",
    "    return mean_computation_time, std_computation_time, mean_grad_descent_time, std_grad_descent_time, mean_memory, std_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE single computation benchmark\n",
      "Computation Time:      0.099 ms (±0.166 ms)\n",
      "Gradient Descent Time: 0.157 ms (±0.053906 ms)\n",
      "Memory Usage:          0.05 MB (±0.16)\n",
      "\n",
      "MSE batch computation benchmark\n",
      "Computation Time:      0.192 ms (±0.305 ms)\n",
      "Gradient Descent Time: 0.164 ms (±0.078221 ms)\n",
      "Memory Usage:          1.68 MB (±5.03)\n",
      "\n",
      "------------------------------------------\n",
      "\n",
      "MAE single computation benchmark\n",
      "Computation Time:      0.051 ms (±0.021 ms)\n",
      "Gradient Descent Time: 0.252 ms (±0.311971 ms)\n",
      "Memory Usage:          0.03 MB (±0.08)\n",
      "\n",
      "MAE batch computation benchmark\n",
      "Computation Time:      0.061 ms (±0.032 ms)\n",
      "Gradient Descent Time: 0.177 ms (±0.078417 ms)\n",
      "Memory Usage:          0.84 MB (±2.52)\n",
      "\n",
      "Benchmarking done.\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "frame_size = 2**16\n",
    "\n",
    "# Mean Squared Error (MSE) Loss\n",
    "def mse_loss(x, y):\n",
    "    return F.mse_loss(x, y)\n",
    "\n",
    "print(\"MSE single computation benchmark\")\n",
    "benchmark_loss_function(mse_loss, input_shape=(1, frame_size), device=device)\n",
    "\n",
    "print(\"MSE batch computation benchmark\")\n",
    "benchmark_loss_function(mse_loss, input_shape=(32, frame_size), device=device)\n",
    "\n",
    "print(\"------------------------------------------\\n\")\n",
    "\n",
    "# Mean Absolute Error (MAE) Loss\n",
    "def mae_loss(x, y):\n",
    "    return F.l1_loss(x, y)\n",
    "\n",
    "print(\"MAE single computation benchmark\")\n",
    "benchmark_loss_function(mae_loss, input_shape=(1, frame_size), device=device)\n",
    "print(\"MAE batch computation benchmark\")\n",
    "benchmark_loss_function(mae_loss, input_shape=(32, frame_size), device=device)\n",
    "\n",
    "print(\"Benchmarking done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSS single computation benchmark:\n",
      "Computation Time:      1.817 ms (±0.882 ms)\n",
      "Gradient Descent Time: 1.144 ms (±0.155149 ms)\n",
      "Memory Usage:          0.03 MB (±0.08)\n",
      "\n",
      "MSS batch computation benchmark:\n",
      "Computation Time:      3.888 ms (±0.314 ms)\n",
      "Gradient Descent Time: 8.485 ms (±0.280359 ms)\n",
      "Memory Usage:          0.85 MB (±2.60)\n",
      "\n",
      "Benchmarking done.\n"
     ]
    }
   ],
   "source": [
    "# Multiscale Spectrogram Loss for comparison\n",
    "def multiscale_fft(signal, scales=[8192, 4096, 2048, 1024, 512, 256, 128], overlap=.75):\n",
    "    stfts = []\n",
    "    for s in scales:\n",
    "        S = torch.stft(\n",
    "            signal,\n",
    "            s,\n",
    "            int(s * (1 - overlap)),\n",
    "            s,\n",
    "            torch.hann_window(s).to(signal),\n",
    "            True,\n",
    "            normalized=True,\n",
    "            return_complex=True,\n",
    "        ).abs()\n",
    "        stfts.append(S)\n",
    "    return stfts\n",
    "\n",
    "def safe_log(x):\n",
    "    return torch.log(x + 1e-7)\n",
    "\n",
    "def multiscale_spectrogram_loss(x, x_hat):\n",
    "    ori_stft = multiscale_fft(x)\n",
    "    rec_stft = multiscale_fft(x_hat)\n",
    "    loss = 0\n",
    "    for s_x, s_y in zip(ori_stft, rec_stft):\n",
    "        lin_loss = (s_x - s_y).abs().mean()\n",
    "        log_loss = (safe_log(s_x) - safe_log(s_y)).abs().mean()\n",
    "        loss = loss + lin_loss + log_loss\n",
    "    return loss\n",
    "\n",
    "# Parameters for TexStat\n",
    "frame_size = 2**16\n",
    "\n",
    "# Running single computation benchmark\n",
    "print(\"MSS single computation benchmark:\")\n",
    "benchmark_loss_function(multiscale_spectrogram_loss, input_shape=(1, frame_size), device=device)\n",
    "\n",
    "# Running batch computation benchmark\n",
    "print(\"MSS batch computation benchmark:\")\n",
    "benchmark_loss_function(multiscale_spectrogram_loss, input_shape=(32, frame_size), device=device)\n",
    "\n",
    "print(\"Benchmarking done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TexStat single computation benchmark:\n",
      "Computation Time:      6.604 ms (±4.834 ms)\n",
      "Gradient Descent Time: 6.047 ms (±0.818343 ms)\n",
      "Memory Usage:          0.29 MB (±0.87)\n",
      "\n",
      "TexStat batch computation benchmark:\n",
      "Computation Time:      93.458 ms (±0.485 ms)\n",
      "Gradient Descent Time: 154.615 ms (±0.401312 ms)\n",
      "Memory Usage:          0.84 MB (±2.52)\n",
      "\n",
      "Benchmarking done.\n"
     ]
    }
   ],
   "source": [
    "# Parameters for TexStat\n",
    "sr, frame_size = 44100, 2**16\n",
    "N_filter_bank = 16\n",
    "M_filter_bank = 6\n",
    "N_moments     = 4\n",
    "alpha         = torch.tensor([10, 1, 1/10, 1/100], device=device)\n",
    "beta  = torch.tensor([1, 1, 1, 1, 1], device=device)\n",
    "new_sr, new_frame_size = sr // 4, frame_size // 4\n",
    "downsampler = torchaudio.transforms.Resample(sr, new_sr).to(device)\n",
    "coch_fb = fb.EqualRectangularBandwidth(frame_size, sr, N_filter_bank, 20, sr // 2)\n",
    "mod_fb  = fb.Logarithmic(new_frame_size, new_sr, M_filter_bank, 10, new_sr // 4)\n",
    "\n",
    "def custom_texstat_loss(x, y, coch_fb, mod_fb, downsampler, N_moments, alpha, beta):\n",
    "    return texstat_loss(x, y, coch_fb, mod_fb, downsampler, N_moments, alpha, beta)\n",
    "\n",
    "# Running single computation benchmark\n",
    "print(\"TexStat single computation benchmark:\")\n",
    "benchmark_loss_function(custom_texstat_loss, input_shape=(1, frame_size), device=device,\n",
    "                        coch_fb=coch_fb, mod_fb=mod_fb, downsampler=downsampler,\n",
    "                        N_moments=N_moments, alpha=alpha, beta=beta)\n",
    "\n",
    "# Running batch computation benchmark\n",
    "print(\"TexStat batch computation benchmark:\")\n",
    "benchmark_loss_function(custom_texstat_loss, input_shape=(32, frame_size), device=device,\n",
    "                        coch_fb=coch_fb, mod_fb=mod_fb, downsampler=downsampler,\n",
    "                        N_moments=N_moments, alpha=alpha, beta=beta)\n",
    "\n",
    "print(\"Benchmarking done.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_texstat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
